[trainer]
; 批大小调至更稳妥的 256（显存更友好，单位时间迭代更快；如显存很充裕可改 512/1024）
batch_size = 256
gamma = 0.95
tau = 0.005
lr_actor  = 0.0003
lr_critic = 0.0003
lr_alpha  = 0.0003

; 三个关键开关
target_entropy = -3.0
use_amp = true
grad_clip = 1.0

; 兼容字段（现在不直接用）
bc_ratio  = 1.2
lambda_bc = 1.0

[imitation_learning]
; 根据我们“先用已有 ORCA demos + 轻量 IL”的建议，收集量与 BC 轮数下调
il_episodes = 100
il_policy = orca
il_epochs = 20
il_learning_rate = 0.0003
safety_space = 0.12

[train]
; 训练端吞吐：更多 updates、较少交互，让 GPU 吃满
train_batches = 128
train_episodes = 1500

; 若并发关闭时的退化参数（并发开时此项影响小）
sample_episodes = 12

; 轻量评估 + 定期全量（与 train.py 中逻辑匹配）
evaluation_interval = 50
eval_full_every = 200
val_subset = 50
train_stat_stride = 5

capacity = 200000
epsilon_start = 0.08
epsilon_end = 0.03
epsilon_decay = 1500

; 降低模型存盘打断频次（I/O 造成停顿）
checkpoint_interval = 250

; 旧字段（当前不使用，但保留以兼容）
target_update_interval = 50

[dagger]
exp_full = 0
exp_anneal = 1
min_prob = 0.0

; ===== 新增：并发采样配置（与 parallel_sampler 对应）=====
[vectorize]
enable = true
num_workers = 2
episodes_per_worker = 2
broadcast_interval = 5
worker_device = cuda:0   ; mamba_ssm 需要 CUDA 前向；显存紧张可先把 num_workers 降到 1


