[trainer]
batch_size = 1024
gamma = 0.95
tau = 0.01                  ; 显著提高目标网络更新频率
lr_actor  = 0.0003
lr_critic = 0.0003
lr_alpha  = 0.0003
target_entropy = -2.0       ; 初始值；train.py 内部会做动态日程
use_amp = true
grad_clip = 1.0
awbc_beta = 2.5             ; 优势加权 BC 的温度（若 train.py 传入则生效）

[imitation_learning]
il_episodes = 100
il_policy = orca
il_epochs = 2000
il_learning_rate = 0.0003
safety_space = 0.12

[train]
train_batches = 80            ; 减少更新步数提升速度
train_episodes = 1500
sample_episodes = 15
target_update_interval = 50
evaluation_interval = 150
capacity = 200000             ; 增加到20万提升稳定性
epsilon_start = 0.08
epsilon_end = 0.03
epsilon_decay = 1500
checkpoint_interval = 500

; 轻量评估相关（新）
val_subset = 50               ; 子集评估的样本数
eval_full_every = 200         ; 每隔多少 episode 做一次全量 val
train_stat_stride = 5         ; 训练期内部快速统计的最小间隔

[vectorize]
enable = true
num_workers = 4               ; 并行 worker 数
episodes_per_worker = 4       ; 每个 worker 采集更多 episode
broadcast_interval = 5
worker_device = cuda:0        ; 关键：worker 前向走 GPU（mamba_ssm 需要）

[dagger]
exp_full = 0
exp_anneal = 1
min_prob = 0.0

