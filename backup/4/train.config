[trainer]
batch_size = 512
gamma = 0.95
tau = 0.005
lr_actor  = 0.0003
lr_critic = 0.0003
lr_alpha  = 0.0003
bc_ratio  = 1.2          ; 轻微加大 BC 正则，稳住早期策略
lambda_bc = 1.0

[imitation_learning]
il_episodes = 1000
il_policy = orca
il_epochs = 2000
il_learning_rate = 0.0003
safety_space = 0.10

[train]
train_batches = 120      ; 每个外层 ep 多训练一点，梯度更平滑
train_episodes = 1500
sample_episodes = 24     ; 采样稍加多，经验更充足
target_update_interval = 50
evaluation_interval = 150
capacity = 100000
epsilon_start = 0.08     ; 少一点随机探索，降低早期撞人
epsilon_end = 0.03
epsilon_decay = 1500
checkpoint_interval = 500

[dagger]
exp_full = 200           ; 专家整集混采的“满强度”阶段拉长
exp_anneal = 900         ; 更慢退火，缓解 100ep 后的“悬崖”
min_prob = 0.25          ; 后期也保留一点点专家稳定收尾

