[trainer]
; 更大的batch size提高训练稳定性
batch_size = 2048
gamma = 0.99  ; 提高折扣因子，更重视长期奖励
tau = 0.005
lr_actor  = 0.0001  ; 降低学习率，提高稳定性
lr_critic = 0.0001
lr_alpha  = 0.0001

; 调整探索参数
target_entropy = -1.0   ; 提高目标熵，增加探索
use_amp = true
grad_clip = 0.5         ; 降低梯度裁剪，防止过度更新

[imitation_learning]
il_episodes = 500       ; 增加专家数据收集
il_policy = orca
il_epochs = 3000        ; 增加BC预训练
il_learning_rate = 0.0001
safety_space = 0.15     ; 增加安全距离

[train]
train_batches = 200     ; 增加每轮训练步数
train_episodes = 2000   ; 增加总训练轮数
sample_episodes = 32    ; 增加采样episode数
target_update_interval = 25  ; 更频繁的目标网络更新
evaluation_interval = 100    ; 更频繁的评估
capacity = 200000       ; 增加回放缓冲区大小
epsilon_start = 0.3     ; 提高初始探索率
epsilon_end = 0.05      ; 提高最终探索率
epsilon_decay = 2000    ; 延长探索衰减
checkpoint_interval = 200

[dagger]
exp_full = 0
exp_anneal = 1
min_prob = 0.0
