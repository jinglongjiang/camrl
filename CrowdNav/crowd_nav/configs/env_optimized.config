[env]
time_limit = 80         ; 减少时间限制，避免长时间无效探索
time_step = 0.25
val_size = 100
test_size = 300
randomize_attributes = true

[reward]
; 调整奖励函数，更平滑的奖励信号
success_reward = 15.0   ; 提高成功奖励
collision_penalty = -5.0 ; 降低碰撞惩罚，避免过度保守

; 更温和的近距惩罚
discomfort_dist = 0.25
discomfort_penalty_factor = 3.0

; 优化shaping奖励
w_prog  = 2.0          ; 降低进度奖励权重
w_goal  = 3.0          ; 降低目标奖励权重
w_coll  = 2.0          ; 降低碰撞奖励权重
w_soc   = 0.5          ; 降低社交奖励权重
soc_dist= 0.6
alpha   = 8.0          ; 降低alpha参数
v_safe  = 0.6          ; 提高安全速度
w_relv  = 0.4          ; 降低相对速度权重
w_time  = 0.02         ; 降低时间权重
w_shape = 0.0
w_align = 0.0
max_dist= 8.0          ; 减少最大距离

; 调整预判参数
w_ttc = 0.4
ttc_thresh = 2.5

; 速度整形
w_speed = 0.1
speed_target = 0.8

; 早停参数
noprog_eps = 0.02
noprog_patience = 20

[sim]
train_val_sim = circle_crossing
test_sim = circle_crossing
square_width = 8        ; 减小环境大小，提高训练效率
circle_radius = 3
human_num = 4           ; 减少人类数量，降低复杂度

[humans]
visible = true
policy = orca
radius = 0.3
v_pref = 0.8           ; 降低人类速度，减少碰撞
sensor = coordinates

[robot]
visible = false
policy = none
radius = 0.3
v_pref = 0.8           ; 降低机器人速度
sensor = coordinates

[orca]
; 更保守的ORCA设置
neighbor_dist = 2.5
max_neighbors = 6
time_horizon = 3.0
time_horizon_obst = 2.5
radius = 0.3
max_speed = 1.0
safety_space = 0.08
label_inflate = 0.12
slow_k = 1.5
eps_noise = 0.02

; TTC制动
ttc_brake = 2.5
brake_min_ratio = 0.4

human_pref_mode = goal
