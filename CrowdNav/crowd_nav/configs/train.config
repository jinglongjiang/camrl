[trainer]
batch_size = 1024
gamma = 0.95
tau = 0.007                 ; 原 0.005，目标网更快跟随
lr_actor  = 0.0003
lr_critic = 0.0003
lr_alpha  = 0.0003
target_entropy = -2.0       ; 初始化值；train.py 内部会做动态日程
use_amp = true
grad_clip = 1.0
awbc_beta = 5.0             ; 优势加权 BC 的温度（代码里有默认同 5.0）

[imitation_learning]
il_episodes = 100           ; 你的实际运行更接近 100，这里按加速版；要更强监督可调回 300
il_policy = orca
il_epochs = 2000
il_learning_rate = 0.0003
safety_space = 0.12

[train]
train_batches = 120
train_episodes = 1500
sample_episodes = 24
target_update_interval = 50
evaluation_interval = 150
capacity = 100000
epsilon_start = 0.08
epsilon_end = 0.03
epsilon_decay = 1500
checkpoint_interval = 500

; 轻量评估相关（新）
val_subset = 50             ; 子集评估的样本数
eval_full_every = 200       ; 每隔多少 episode 做一次全量 val
train_stat_stride = 5       ; 训练期内部快速统计的最小间隔

[vectorize]
enable = true
num_workers = 3             ; 显存允许可设 3~4
episodes_per_worker = 3
broadcast_interval = 5
worker_device = cuda:0      ; 关键：worker 前向走 GPU（mamba_ssm 需要）

[dagger]
exp_full = 0
exp_anneal = 1
min_prob = 0.0
